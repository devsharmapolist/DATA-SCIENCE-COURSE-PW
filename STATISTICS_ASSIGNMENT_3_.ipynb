{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIQ6IjIhvvfgm0pB0Q97/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devsharmapolist/DATA-SCIENCE-COURSE-PW/blob/main/STATISTICS_ASSIGNMENT_3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the properties of the F-distribution.\n",
        "\n",
        "The **F-distribution** is a continuous probability distribution that arises frequently in statistical analysis, especially in variance comparisons and hypothesis testing (e.g., ANOVA and regression analysis). It is named after Sir Ronald Fisher.\n",
        "\n",
        "### **Properties of the F-Distribution**\n",
        "1. **Non-Negative Values**  \n",
        "   - The F-distribution is always **non-negative**, meaning \\( F \\geq 0 \\). This is because it is a ratio of variances (which are always non-negative).\n",
        "\n",
        "2. **Asymmetry (Right-Skewed)**  \n",
        "   - The F-distribution is **positively skewed** (right-skewed).  \n",
        "   - As the degrees of freedom increase, the distribution becomes more **symmetric** and approaches a normal shape.\n",
        "\n",
        "3. **Shape Depends on Two Degrees of Freedom**  \n",
        "   - The F-distribution is determined by two **degrees of freedom** (df):  \n",
        "     - \\( d_1 \\) (numerator degrees of freedom)  \n",
        "     - \\( d_2 \\) (denominator degrees of freedom)  \n",
        "   - The shape varies depending on these parameters.\n",
        "\n",
        "4. **Mean of the F-Distribution**  \n",
        "   - The mean is given by:  \n",
        "     \\[\n",
        "     E(F) = \\frac{d_2}{d_2 - 2}, \\quad \\text{for } d_2 > 2\n",
        "     \\]\n",
        "   - If \\( d_2 \\leq 2 \\), the mean is **undefined**.\n",
        "\n",
        "5. **Variance of the F-Distribution**  \n",
        "   - The variance is given by:  \n",
        "     \\[\n",
        "     \\text{Var}(F) = \\frac{2 d_2^2 (d_1 + d_2 - 2)}{d_1 (d_2 - 2)^2 (d_2 - 4)}, \\quad \\text{for } d_2 > 4\n",
        "     \\]\n",
        "   - If \\( d_2 \\leq 4 \\), the variance is **undefined**.\n",
        "\n",
        "6. **Right-Tailed Distribution**  \n",
        "   - The F-distribution is **not symmetric** and has a long right tail.\n",
        "   - It is used in **one-tailed tests**, especially for comparing variances.\n",
        "\n",
        "7. **Special Case: Relationship with Chi-Square and t-Distributions**  \n",
        "   - If \\( X_1 \\sim \\chi^2(d_1) \\) and \\( X_2 \\sim \\chi^2(d_2) \\) are independent chi-square-distributed random variables, then:  \n",
        "     \\[\n",
        "     F = \\frac{X_1 / d_1}{X_2 / d_2} \\sim F(d_1, d_2)\n",
        "     \\]\n",
        "   - The F-distribution is related to the **t-distribution** as well:  \n",
        "     \\[\n",
        "     F(1, d_2) = t^2(d_2)\n",
        "     \\]\n",
        "   - That is, an F-distribution with a numerator degree of freedom \\( d_1 = 1 \\) is equivalent to the square of a t-distributed random variable.\n",
        "\n",
        "8. **Additivity Does Not Hold**  \n",
        "   - Unlike the normal or chi-square distributions, the sum of two independent F-distributed variables does **not** follow an F-distribution.\n",
        "\n",
        "### **Conclusion**\n",
        "The F-distribution is primarily used in hypothesis testing, particularly in ANOVA, regression analysis, and tests for equality of variances (such as Levene’s test and Bartlett’s test). It is right-skewed and depends on two degrees of freedom, making its shape flexible based on the given dataset."
      ],
      "metadata": {
        "id": "K1ms2BwYv4C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "The **F-distribution** is primarily used in statistical tests where variances are compared, particularly in hypothesis testing for variance ratios. It is appropriate for these tests because it arises naturally when analyzing the ratio of two independent sample variances.\n",
        "\n",
        "### **Statistical Tests Using the F-Distribution**\n",
        "1. **Analysis of Variance (ANOVA)**  \n",
        "   - **Purpose**: ANOVA is used to compare the means of three or more groups by analyzing their variances.  \n",
        "   - **Why F-distribution?**  \n",
        "     - The F-test in ANOVA compares the **between-group variance** to the **within-group variance**.  \n",
        "     - If the ratio is significantly larger than 1, it suggests that at least one group mean differs from the others.\n",
        "   - **Formula**:  \n",
        "     \\[\n",
        "     F = \\frac{\\text{Variance between groups}}{\\text{Variance within groups}}\n",
        "     \\]\n",
        "   - **Example**: Comparing the average scores of students in different sections of a class.\n",
        "\n",
        "2. **Regression Analysis (Overall Significance Test for Multiple Regression)**  \n",
        "   - **Purpose**: Tests whether at least one predictor variable in a multiple regression model significantly explains the variation in the dependent variable.  \n",
        "   - **Why F-distribution?**  \n",
        "     - The F-test in regression compares the **explained variance** (by the regression model) to the **unexplained variance** (error term).\n",
        "   - **Formula**:  \n",
        "     \\[\n",
        "     F = \\frac{\\text{Explained variance (due to regression)}}{\\text{Unexplained variance (due to error)}}\n",
        "     \\]\n",
        "   - **Example**: Testing if advertising spending and price influence product sales.\n",
        "\n",
        "3. **Comparing Two Population Variances (F-Test for Variance Ratio)**  \n",
        "   - **Purpose**: Compares the variances of two independent populations to check if they are significantly different.  \n",
        "   - **Why F-distribution?**  \n",
        "     - Since variance follows a chi-square distribution, the ratio of two variances follows an F-distribution.  \n",
        "     - Used in **homogeneity of variance tests** (Levene’s test, Bartlett’s test).\n",
        "   - **Formula**:  \n",
        "     \\[\n",
        "     F = \\frac{\\sigma_1^2}{\\sigma_2^2}\n",
        "     \\]\n",
        "     where \\( \\sigma_1^2 \\) and \\( \\sigma_2^2 \\) are the sample variances.\n",
        "   - **Example**: Testing if the volatility of stock returns differs between two markets.\n",
        "\n",
        "4. **Two-Way ANOVA**  \n",
        "   - **Purpose**: Extends one-way ANOVA to test the effect of **two independent variables** and their interaction on a dependent variable.  \n",
        "   - **Why F-distribution?**  \n",
        "     - Used to assess the **main effects** and **interaction effects** between the factors.\n",
        "   - **Example**: Studying how both **gender** and **study method** influence exam performance.\n",
        "\n",
        "5. **Testing Nested Regression Models (F-Test in Model Comparison)**  \n",
        "   - **Purpose**: Determines whether adding more predictors to a regression model significantly improves its fit.  \n",
        "   - **Why F-distribution?**  \n",
        "     - The test compares the sum of squared residuals from two models:  \n",
        "       - A **full model** with all predictors  \n",
        "       - A **reduced model** with fewer predictors  \n",
        "     - If adding predictors significantly reduces residual variance, the F-test confirms model improvement.\n",
        "   - **Example**: Checking if adding a new variable (e.g., social media engagement) improves a sales prediction model.\n",
        "\n",
        "### **Why is the F-Distribution Appropriate?**\n",
        "1. **Based on Variance Ratios**  \n",
        "   - Many statistical tests rely on comparing **variances** (e.g., ANOVA, regression, variance comparison tests).\n",
        "   - The F-distribution naturally arises when forming the ratio of two independent chi-square-distributed variances.\n",
        "\n",
        "2. **One-Tailed Test Characteristic**  \n",
        "   - The F-distribution is **right-skewed** and **one-tailed**, making it suitable for tests where we check if one variance is significantly greater than another.\n",
        "\n",
        "3. **Degrees of Freedom Consideration**  \n",
        "   - The shape of the F-distribution depends on **degrees of freedom**, which aligns with the structure of ANOVA, regression, and variance tests.\n",
        "\n",
        "### **Conclusion**\n",
        "The F-distribution is widely used in statistical tests that involve variance comparisons, such as **ANOVA, regression analysis, variance ratio tests, and model comparisons**. Its properties make it a natural fit for evaluating whether groups differ significantly in variability or overall model performance."
      ],
      "metadata": {
        "id": "GGLbIogOwI2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?\n",
        "\n",
        "When conducting an **F-test to compare the variances** of two populations, several key assumptions must be satisfied to ensure the validity of the test results. These assumptions include:\n",
        "\n",
        "### **1. The Populations are Normally Distributed**\n",
        "- The **F-test is highly sensitive to non-normality**.  \n",
        "- Both populations being compared should **follow a normal distribution**.  \n",
        "- If the data is skewed or has outliers, the F-test may lead to incorrect conclusions.\n",
        "\n",
        "**How to check?**  \n",
        "- Use a **histogram**, **Q-Q plot**, or **Shapiro-Wilk test** to verify normality.  \n",
        "- If normality is violated, use a **non-parametric test** like Levene’s test or Bartlett’s test.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The Samples are Independent**\n",
        "- The two samples must be **randomly selected** and **independent** of each other.  \n",
        "- This means that observations in one sample should not influence the observations in the other.\n",
        "\n",
        "**How to check?**  \n",
        "- Verify the study design ensures **no dependency** between groups.  \n",
        "- If the samples are paired (e.g., before-and-after measurements), a **paired t-test** might be more appropriate.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The Populations are Independent**\n",
        "- The two populations themselves should be independent, meaning that one population's variability does not influence the other.\n",
        "\n",
        "**How to check?**  \n",
        "- Ensure the samples come from distinct groups with **no overlapping influence**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. The Samples are Randomly Selected**\n",
        "- The data should be collected using a **random sampling method** to ensure unbiased results.\n",
        "\n",
        "**How to check?**  \n",
        "- Confirm that the selection process was **random and free from systematic bias**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. The Data is Measured on an Interval or Ratio Scale**\n",
        "- The F-test is only valid for **quantitative data** measured on an **interval or ratio scale** (e.g., height, weight, temperature, time).  \n",
        "- It is **not valid for categorical data**.\n",
        "\n",
        "**How to check?**  \n",
        "- Ensure that the data consists of numerical values that allow meaningful variance calculations.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. The Larger Variance is Placed in the Numerator**\n",
        "- To ensure the F-value is always **≥ 1**, the sample with the larger variance should be placed in the numerator:\n",
        "  \\[\n",
        "  F = \\frac{S_1^2}{S_2^2}, \\quad \\text{where } S_1^2 > S_2^2\n",
        "  \\]\n",
        "- This ensures that the test remains **right-tailed**.\n",
        "\n",
        "**How to check?**  \n",
        "- Compare the sample variances and assign the larger one to the numerator.\n",
        "\n",
        "---\n",
        "\n",
        "### **Implications of Violating Assumptions**\n",
        "| **Assumption Violated** | **Consequence** | **Alternative Test** |\n",
        "|-----------------|------------------|----------------|\n",
        "| Normality | Inflated Type I error (false positives) | Levene’s test (robust to non-normality) |\n",
        "| Independence of samples | Biased variance estimates | Use a paired test if samples are related |\n",
        "| Random sampling | Non-generalizable results | Ensure proper experimental design |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "The **F-test for comparing variances** requires **normality, independence, random sampling, and quantitative data**. If any of these assumptions are violated, alternative tests like **Levene’s test** (for non-normal data) or **Bartlett’s test** (for multiple groups) should be used."
      ],
      "metadata": {
        "id": "pO1kyibcwTrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "### **Purpose of ANOVA**  \n",
        "**Analysis of Variance (ANOVA)** is a statistical method used to determine whether there are significant differences between the means of **three or more independent groups**. It helps answer the question:  \n",
        "\n",
        "*\"Do at least one of the group means differ significantly from the others?\"*  \n",
        "\n",
        "Instead of performing multiple t-tests (which increases the chance of Type I error), ANOVA **compares all group means simultaneously** using variance analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### **Difference Between ANOVA and t-Test**  \n",
        "\n",
        "| **Feature** | **ANOVA** | **t-Test** |\n",
        "|------------|----------|------------|\n",
        "| **Purpose** | Compares means of **three or more** groups | Compares means of **two** groups |\n",
        "| **Hypothesis** | \\( H_0: \\mu_1 = \\mu_2 = \\mu_3 = ... = \\mu_k \\) (All group means are equal) | \\( H_0: \\mu_1 = \\mu_2 \\) (Two group means are equal) |\n",
        "| **Test Statistic** | **F-statistic** \\( F = \\frac{\\text{Variance between groups}}{\\text{Variance within groups}} \\) | **t-statistic** \\( t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\text{Standard Error}} \\) |\n",
        "| **Number of Groups Compared** | **3 or more** | **Only 2** |\n",
        "| **Error Control** | Avoids multiple testing errors | Increases error probability if multiple tests are performed |\n",
        "| **Types** | One-way ANOVA, Two-way ANOVA, MANOVA | Independent t-test, Paired t-test |\n",
        "| **Post-hoc Analysis** | Requires further tests (e.g., Tukey’s HSD) to identify which groups differ | No post-hoc test needed |\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences in When to Use Each**  \n",
        "- **Use a t-test** when comparing **only two groups** (e.g., Test scores of Group A vs. Group B).  \n",
        "- **Use ANOVA** when comparing **three or more groups** (e.g., Test scores of Group A, Group B, and Group C).  \n",
        "\n",
        "If ANOVA finds a significant difference, **post-hoc tests** (like Tukey’s HSD) are required to determine which specific groups differ.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "- ANOVA is **generalized** for multiple groups, whereas the t-test is **limited to two groups**.  \n",
        "- ANOVA is **more efficient** in controlling errors when comparing multiple means.  \n",
        "- When comparing **exactly two groups**, ANOVA and the t-test give the **same results**."
      ],
      "metadata": {
        "id": "GBmrg4SAwb1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups.\n",
        "\n",
        "### **When and Why to Use One-Way ANOVA Instead of Multiple t-Tests**  \n",
        "\n",
        "#### **When to Use One-Way ANOVA?**  \n",
        "You should use a **one-way ANOVA** when:  \n",
        "1. You need to **compare the means** of **three or more independent groups**.  \n",
        "2. The groups are defined by a **single independent variable (factor)** with different levels (e.g., treatment groups, age groups, or education levels).  \n",
        "3. The assumptions of ANOVA are met:  \n",
        "   - The populations are normally distributed.  \n",
        "   - The samples are independent.  \n",
        "   - The variances of the groups are equal (homogeneity of variance).  \n",
        "\n",
        "**Example:**  \n",
        "A researcher wants to compare the average exam scores of students from **three different teaching methods** (A, B, and C). Using one-way ANOVA is the best approach.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Why Use One-Way ANOVA Instead of Multiple t-Tests?**  \n",
        "\n",
        "##### **1. Avoids Inflated Type I Error (False Positives)**  \n",
        "- Each **t-test** has a **5% chance** of incorrectly rejecting \\( H_0 \\) (if \\( \\alpha = 0.05 \\)).  \n",
        "- If you conduct multiple t-tests, the **overall error rate increases**.  \n",
        "- **Example:**  \n",
        "  - If comparing **three** groups (A, B, and C), you need **three** t-tests:  \n",
        "    1. A vs. B  \n",
        "    2. A vs. C  \n",
        "    3. B vs. C  \n",
        "  - If comparing **four** groups, you need **six** t-tests.  \n",
        "  - The more tests you conduct, the higher the chance of making a **Type I error**.  \n",
        "  - **One-way ANOVA** controls for this error by testing all groups in a **single test**.\n",
        "\n",
        "##### **2. More Efficient and Comprehensive**  \n",
        "- Instead of conducting multiple pairwise t-tests, one-way ANOVA **compares all group means simultaneously**.  \n",
        "- It **identifies whether at least one group differs**, without the need for multiple calculations.  \n",
        "- If ANOVA detects a significant difference, **post-hoc tests** (e.g., Tukey’s HSD) can determine which groups are different.\n",
        "\n",
        "##### **3. Accounts for Variability Better**  \n",
        "- One-way ANOVA considers **both within-group and between-group variability**, making it a **more robust** statistical method.  \n",
        "- **t-tests** only compare individual group pairs, ignoring overall variability.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "- **Use one-way ANOVA** when comparing **three or more groups** to prevent multiple testing errors and ensure accurate results.  \n",
        "- If ANOVA finds a significant difference, follow up with **post-hoc tests** to determine which groups differ.  \n",
        "- **Multiple t-tests increase Type I error**, making them unreliable for multiple comparisons."
      ],
      "metadata": {
        "id": "O6pR7m6CwmHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?\n",
        "### **Partitioning of Variance in ANOVA**  \n",
        "\n",
        "In **one-way ANOVA**, the total variability in the data is divided into two components:  \n",
        "1. **Between-Group Variance (Variance due to Treatment/Factor)** – Measures the variation **between** the different groups.  \n",
        "2. **Within-Group Variance (Error Variance/Residual Variance)** – Measures the variation **within** each group.\n",
        "\n",
        "This partitioning helps in calculating the **F-statistic**, which determines whether the group means are significantly different.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Total Variance (Total Sum of Squares, SST)**  \n",
        "- Represents the overall variation in the dataset.  \n",
        "- It is the sum of squared differences between each observation and the **overall mean** \\( \\bar{X} \\).  \n",
        "\\[\n",
        "SS_T = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X})^2\n",
        "\\]  \n",
        "where:  \n",
        "- \\( X_{ij} \\) is the observation in group \\( i \\), subject \\( j \\).  \n",
        "- \\( \\bar{X} \\) is the overall mean of all data points.  \n",
        "- \\( k \\) is the number of groups, and \\( n_i \\) is the number of observations in group \\( i \\).  \n",
        "\n",
        "---\n",
        "\n",
        "### **2. Between-Group Variance (Sum of Squares Between, SSB or SSTr)**  \n",
        "- Measures how much the **group means differ from the overall mean**.  \n",
        "- It captures the variation **due to the treatment effect** (if any).  \n",
        "\\[\n",
        "SS_B = \\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X})^2\n",
        "\\]  \n",
        "where:  \n",
        "- \\( \\bar{X}_i \\) is the mean of group \\( i \\).  \n",
        "- \\( n_i \\) is the number of observations in group \\( i \\).  \n",
        "- The larger this value, the greater the difference between group means.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Within-Group Variance (Sum of Squares Within, SSW or SSE)**  \n",
        "- Measures how much **individual observations vary within each group**.  \n",
        "- It represents random variation **not explained by the factor**.  \n",
        "\\[\n",
        "SS_W = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2\n",
        "\\]  \n",
        "where:  \n",
        "- \\( X_{ij} \\) is the individual observation in group \\( i \\), subject \\( j \\).  \n",
        "- \\( \\bar{X}_i \\) is the mean of group \\( i \\).  \n",
        "- If this value is large, it means there is **high variability within groups**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Calculation of the F-Statistic**  \n",
        "The **F-statistic** is calculated as the ratio of **between-group variance to within-group variance**:\n",
        "\n",
        "\\[\n",
        "F = \\frac{\\text{Mean Square Between Groups (MSB)}}{\\text{Mean Square Within Groups (MSW)}}\n",
        "\\]\n",
        "\n",
        "where:  \n",
        "- **Mean Square Between (MSB)**:  \n",
        "  \\[\n",
        "  MSB = \\frac{SS_B}{df_B} = \\frac{SS_B}{k - 1}\n",
        "  \\]\n",
        "  Measures variance **due to group differences**.  \n",
        "\n",
        "- **Mean Square Within (MSW)**:  \n",
        "  \\[\n",
        "  MSW = \\frac{SS_W}{df_W} = \\frac{SS_W}{N - k}\n",
        "  \\]\n",
        "  Measures variance **within groups**.  \n",
        "\n",
        "- **Degrees of freedom**:  \n",
        "  - \\( df_B = k - 1 \\) (number of groups - 1)  \n",
        "  - \\( df_W = N - k \\) (total observations - number of groups)  \n",
        "\n",
        "- **Interpretation of F-Value**:  \n",
        "  - **If \\( F \\) is large** → The between-group variance is significantly larger than the within-group variance → At least one group mean is different → **Reject \\( H_0 \\)**.  \n",
        "  - **If \\( F \\) is close to 1** → The group differences are due to chance → **Fail to reject \\( H_0 \\)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "- **ANOVA partitions variance** into **between-group variance** (due to treatment) and **within-group variance** (random error).  \n",
        "- The **F-statistic** is the ratio of these variances.  \n",
        "- A **higher F-value** indicates significant differences between group means."
      ],
      "metadata": {
        "id": "Tx6-52OvwtIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "### **Comparison of Classical (Frequentist) ANOVA and Bayesian ANOVA**  \n",
        "\n",
        "ANOVA can be approached using **frequentist (classical) statistics** or **Bayesian statistics**. These two frameworks differ in their treatment of uncertainty, parameter estimation, and hypothesis testing.  \n",
        "\n",
        "---\n",
        "\n",
        "### **1. Treatment of Uncertainty**  \n",
        "| **Aspect** | **Frequentist ANOVA** | **Bayesian ANOVA** |\n",
        "|------------|----------------------|---------------------|\n",
        "| **Definition of Probability** | Probability is the **long-run frequency** of an event occurring. | Probability represents **degree of belief** in a hypothesis based on prior knowledge. |\n",
        "| **Uncertainty Handling** | Uncertainty is **expressed through p-values**, which indicate how extreme the data is under \\(H_0\\). | Uncertainty is **quantified using probability distributions** over parameters. |\n",
        "| **Interpretation** | Results are interpreted in terms of rejecting or failing to reject \\(H_0\\). | Results provide **posterior distributions** for the effect sizes and model parameters. |\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Parameter Estimation**  \n",
        "| **Aspect** | **Frequentist ANOVA** | **Bayesian ANOVA** |\n",
        "|------------|----------------------|---------------------|\n",
        "| **Parameter Estimation** | Estimates parameters (e.g., means, variances) using **point estimates** (e.g., sample means and variances). | Uses **Bayes’ theorem** to update a prior distribution with observed data to obtain a **posterior distribution**. |\n",
        "| **Confidence vs. Credibility** | Uses **confidence intervals (CI)** to estimate the range where parameters likely fall. | Uses **credible intervals (CrI)**, which give the actual probability that the parameter lies within the interval. |\n",
        "| **Flexibility** | Assumes fixed effects and normality of residuals. | Allows incorporation of **prior knowledge** and works well with complex hierarchical models. |\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Hypothesis Testing**  \n",
        "| **Aspect** | **Frequentist ANOVA** | **Bayesian ANOVA** |\n",
        "|------------|----------------------|---------------------|\n",
        "| **Null Hypothesis (\\(H_0\\))** | Assumes no difference in means (\\(\\mu_1 = \\mu_2 = \\dots = \\mu_k\\)). | Computes the probability of different hypotheses based on observed data. |\n",
        "| **Test Statistic** | Uses the **F-statistic** to compare between-group and within-group variance. | Uses **Bayes factors** to compare the relative strength of models (e.g., \\(H_0\\) vs. \\(H_1\\)). |\n",
        "| **p-Values** | A small p-value (< 0.05) suggests rejecting \\(H_0\\) (but does not measure probability of \\(H_0\\) being true). | Bayes factors directly quantify **how much more likely** one hypothesis is over another. |\n",
        "| **Decision Making** | If p-value is low, reject \\(H_0\\); otherwise, fail to reject \\(H_0\\). | If **Bayes factor (BF) > 3**, it favors the alternative hypothesis (\\(H_1\\)). If BF < 1/3, it supports \\(H_0\\). |\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Interpretation of Results**  \n",
        "| **Aspect** | **Frequentist ANOVA** | **Bayesian ANOVA** |\n",
        "|------------|----------------------|---------------------|\n",
        "| **What does a significant result mean?** | There is enough evidence to reject \\(H_0\\), but it does not quantify the probability that \\(H_0\\) is true. | Directly estimates the probability that \\(H_0\\) or \\(H_1\\) is true. |\n",
        "| **Effect Size Consideration** | Requires post-hoc tests to estimate effect size (e.g., Cohen’s \\(d\\)). | Provides **posterior distributions of effect sizes**, which are more informative. |\n",
        "| **Practicality** | Commonly used and easy to compute with standard statistical software. | More computationally intensive but gives richer insights. |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Each Approach?**  \n",
        "| **Scenario** | **Use Frequentist ANOVA** | **Use Bayesian ANOVA** |\n",
        "|-------------|------------------------|------------------------|\n",
        "| **You need a quick decision** | When making standard hypothesis tests (e.g., clinical trials, A/B testing). | When prior information is available and results should be expressed probabilistically. |\n",
        "| **You have prior knowledge** | Less useful, as it does not incorporate prior beliefs. | Useful when prior knowledge about parameters exists. |\n",
        "| **Data is small or complex** | Large sample sizes are needed for accurate results. | Works well with small samples and hierarchical models. |\n",
        "| **Computational resources** | Requires minimal computation. | Computationally intensive but provides richer insights. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**  \n",
        "- **Frequentist ANOVA** relies on p-values and the F-test to determine significance but does not directly quantify probability.  \n",
        "- **Bayesian ANOVA** provides a probabilistic interpretation using posterior distributions and Bayes factors, making it more flexible and informative.  \n",
        "- Bayesian methods are useful when prior knowledge is available or when working with small or complex datasets, while frequentist ANOVA remains a standard choice for straightforward hypothesis testing."
      ],
      "metadata": {
        "id": "2JU_qRBJw4tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "V Profession A: [48, 52, 55, 60, 62'\n",
        "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "UIxmtz6BxCeP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sWDeXEVnBe4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb881c13-2ff5-4863-d7f0-a932f0703371"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.089171974522293, 0.49304859900533904)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Given income data for two professions\n",
        "profession_A = np.array([48, 52, 55, 60, 62])\n",
        "profession_B = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate sample variances\n",
        "var_A = np.var(profession_A, ddof=1)  # Unbiased estimator (sample variance)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "# Compute the F-statistic\n",
        "F_statistic = var_A / var_B\n",
        "\n",
        "# Degrees of freedom\n",
        "df_A = len(profession_A) - 1\n",
        "df_B = len(profession_B) - 1\n",
        "\n",
        "# Compute p-value (two-tailed test)\n",
        "p_value = 2 * min(stats.f.cdf(F_statistic, df_A, df_B), 1 - stats.f.cdf(F_statistic, df_A, df_B))\n",
        "\n",
        "# Output results\n",
        "F_statistic, p_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "V Region A: [160, 162, 165, 158, 164'\n",
        "V Region B: [172, 175, 170, 168, 174'\n",
        "V Region C: [180, 182, 179, 185, 183'\n",
        "V Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "28kog1s5xKsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given height data for three regions\n",
        "region_A = np.array([160, 162, 165, 158, 164])\n",
        "region_B = np.array([172, 175, 170, 168, 174])\n",
        "region_C = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "# Output results\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkvxXewrxSj_",
        "outputId": "29e15177-3fad-48d0-a407-5c6e1e8ed6e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}