{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh5l3ywIzqQpV8SMqoEdFc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devsharmapolist/DATA-SCIENCE-COURSE-PW/blob/main/Neural_Network_Practical_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How do you create a simple perceptron for basic binary classification?"
      ],
      "metadata": {
        "id": "FWWmyxD0iEWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaaM52oph3KF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Perceptron Model\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.1):\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = np.dot(self.weights, x) + self.bias\n",
        "        return step(z)\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for xi, yi in zip(X, y):\n",
        "                y_pred = self.predict(xi)\n",
        "                error = yi - y_pred\n",
        "                self.weights += self.lr * error * xi\n",
        "                self.bias += self.lr * error\n",
        "\n",
        "# Training data for OR gate\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([0, 1, 1, 1])  # OR output\n",
        "\n",
        "# Train the perceptron\n",
        "p = Perceptron(input_size=2)\n",
        "p.train(X, y, epochs=10)\n",
        "\n",
        "# Test\n",
        "print(\"Predictions:\")\n",
        "for x in X:\n",
        "    print(f\"{x} => {p.predict(x)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) How can you build a neural network with one hidden layer using Keras?"
      ],
      "metadata": {
        "id": "-_nVK-GSiN1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Sample dataset (X: features, y: binary labels)\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 1, 1, 0])  # XOR problem (for illustration)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add hidden layer with 4 neurons and ReLU activation\n",
        "model.add(Dense(units=4, activation='relu', input_shape=(2,)))\n",
        "\n",
        "# Add output layer with sigmoid activation (for binary classification)\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Test predictions\n",
        "print(\"Predictions:\")\n",
        "print(model.predict(X))\n"
      ],
      "metadata": {
        "id": "5Chw4wdSi1YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) How do you initialize weights using the Xavier (Glorot) initialization method in Keras?"
      ],
      "metadata": {
        "id": "KH2oeDU2jG8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Hidden layer with Xavier (Glorot) uniform initialization\n",
        "model.add(Dense(units=8, activation='relu', input_shape=(4,), kernel_initializer=glorot_uniform()))\n",
        "\n",
        "# Output layer (binary classification)\n",
        "model.add(Dense(units=1, activation='sigmoid', kernel_initializer=glorot_uniform()))\n"
      ],
      "metadata": {
        "id": "f9AtsZeDjHzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) How can you apply different activation functions in a neural network in Keras?"
      ],
      "metadata": {
        "id": "Zs7kizdsjRzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer (implicitly included)\n",
        "# Hidden layer 1 with ReLU\n",
        "model.add(Dense(units=64, activation='relu', input_shape=(10,)))\n",
        "\n",
        "# Hidden layer 2 with Tanh\n",
        "model.add(Dense(units=32, activation='tanh'))\n",
        "\n",
        "# Hidden layer 3 with Sigmoid\n",
        "model.add(Dense(units=16, activation='sigmoid'))\n",
        "\n",
        "# Output layer for binary classification (Sigmoid)\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "jUdaL1B6jb0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) How do you add dropout to a neural network model to prevent overfitting?"
      ],
      "metadata": {
        "id": "itDsi5RLjcVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input and first hidden layer\n",
        "model.add(Dense(128, activation='relu', input_shape=(100,)))\n",
        "\n",
        "# Apply 50% dropout\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Second hidden layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Apply 30% dropout\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output layer for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "bmmb9LdXjtf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) How do you manually implement forward propagation in a simple neural network?"
      ],
      "metadata": {
        "id": "YseTxxekj_Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample input (one example with 2 features)\n",
        "X = np.array([[0.5], [0.2]])\n",
        "\n",
        "# Weights and biases for hidden layer\n",
        "W1 = np.array([[0.1, 0.4],\n",
        "               [0.3, 0.7]])   # shape (2, 2)\n",
        "b1 = np.array([[0.1], [0.2]])  # shape (2, 1)\n",
        "\n",
        "# Weights and bias for output layer\n",
        "W2 = np.array([[0.6, 0.9]])  # shape (1, 2)\n",
        "b2 = np.array([[0.3]])       # shape (1, 1)\n",
        "\n",
        "# Activation functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Forward pass\n",
        "\n",
        "# Hidden layer\n",
        "Z1 = np.dot(W1, X) + b1\n",
        "A1 = relu(Z1)\n",
        "\n",
        "# Output layer\n",
        "Z2 = np.dot(W2, A1) + b2\n",
        "A2 = sigmoid(Z2)\n",
        "\n",
        "print(\"Output:\", A2)\n"
      ],
      "metadata": {
        "id": "UJZhYhfvkDj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) How do you add batch normalization to a neural network model in Keras?"
      ],
      "metadata": {
        "id": "V0_FNfVYkIbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input and first hidden layer\n",
        "model.add(Dense(64, input_shape=(100,)))              # Linear\n",
        "model.add(BatchNormalization())                       # Normalize\n",
        "model.add(Activation('relu'))                         # Apply activation\n",
        "\n",
        "# Second hidden layer\n",
        "model.add(Dense(32))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Output layer for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "xLuEOaUykYuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) How can you visualize the training process with accuracy and loss curves?\n"
      ],
      "metadata": {
        "id": "dtX_1kptkdIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Dummy model for demonstration\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(100,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Dummy training data\n",
        "import numpy as np\n",
        "X_train = np.random.rand(1000, 100)\n",
        "y_train = np.random.randint(0, 2, size=(1000,))\n",
        "\n",
        "# Train the model and store the history\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=32)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cEglp6iwkj_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?"
      ],
      "metadata": {
        "id": "-Z-__GR5lLJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Example: clipping gradients by norm\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
        "\n",
        "# Or: clipping gradients by value\n",
        "# optimizer = Adam(learning_rate=0.001, clipvalue=0.5)\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(100,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model with clipping\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "l75crgFmlL0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) How can you create a custom loss function in Keras?"
      ],
      "metadata": {
        "id": "jAwsrVFGlTQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def custom_mape(y_true, y_pred):\n",
        "    diff = tf.abs((y_true - y_pred) / tf.clip_by_value(tf.abs(y_true), 1e-7, tf.float32.max))\n",
        "    return tf.reduce_mean(diff) * 100\n"
      ],
      "metadata": {
        "id": "d2JeMBq0lT1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11) How can you visualize the structure of a neural network model in Keras?"
      ],
      "metadata": {
        "id": "u0oZYnt1lbCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# 1. Build the model\n",
        "model = Sequential([\n",
        "    Dense(128, input_shape=(64,), activation='relu', name='input_layer'),\n",
        "    Dense(64, activation='relu', name='hidden_layer_1'),\n",
        "    Dense(32, activation='relu', name='hidden_layer_2'),\n",
        "    Dense(1, activation='sigmoid', name='output_layer')\n",
        "])\n",
        "\n",
        "# 2. Print summary\n",
        "model.summary()\n",
        "\n",
        "# 3. Plot and save the model architecture as an image\n",
        "plot_model(model, to_file='model_structure.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "C9llei3TlgDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}